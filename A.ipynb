{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d09e3c3-7b4d-4999-ad41-1d5bc27b38c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae153630-8440-4fcd-979d-7b88f6e1684b",
   "metadata": {},
   "source": [
    "                                      Assigment ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326ce2d-a2db-47cc-acbf-16eeb26952c1",
   "metadata": {},
   "source": [
    " Question 1: What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40083569-7a98-4b3a-8ef2-0541989aa113",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Information Gain measures how much a feature reduces uncertainty (entropy) about the target variable, and decision trees use it to select the best feature to split a node at each step. The feature with the highest Information Gain is chosen because it creates the most \"pure\" or homogenous child nodes, leading to a more accurate classification. [1, 2, 3]  \n",
    "What is Information Gain? \n",
    "\n",
    "‚Ä¢ Definition: Information Gain is a metric used to select the most informative feature for splitting a dataset in a decision tree. [1, 4]  \n",
    "‚Ä¢ How it works: It quantifies the reduction in entropy (a measure of impurity or uncertainty) of the target variable when the data is partitioned by a particular feature. [1, 2, 3]  \n",
    "‚Ä¢ Formula: $Gain(S, A) = Entropy(S) - \\sum_{v}^{A} \\frac{|S_v|}{|S|} Entropy(S_v)$. [5]  \n",
    "\t‚Ä¢ $S$ is the dataset. \n",
    "\t‚Ä¢ $A$ is an attribute (feature). \n",
    "\t‚Ä¢ $v$ is a value of the attribute $A$. \n",
    "\t‚Ä¢ $Entropy(S)$ is the entropy of the original dataset. \n",
    "\t‚Ä¢ $Entropy(S_v)$ is the entropy of the subset of data where attribute $A$ has value $v$. \n",
    "\t‚Ä¢ $\\frac{|S_v|}{|S|}$ is the proportion of the data that has value $v$ for attribute $A$. [5]  \n",
    "\n",
    "How it's used in decision trees \n",
    "\n",
    "1. Initial Split: At the root node, Information Gain is calculated for every feature. \n",
    "2. Feature Selection: The feature with the highest Information Gain is chosen as the first split, as it provides the most information to separate the data classes. \n",
    "3. Recursive Splitting: This process is repeated recursively for each child node. At each new node, the algorithm considers only the remaining features and selects the one with the highest Information Gain to split the data further. \n",
    "4. Termination: The process continues until a stopping criterion is met, such as a node containing only data points from a single class, or a maximum tree depth is reached. [1, 3, 4, 6, 7, 8, 9, 10, 11, 12]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69070a18-87c7-4901-a0f5-c93c6120616a",
   "metadata": {},
   "source": [
    "Question 2: What is the difference between Gini Impurity and Entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e000ee-6965-4925-aa77-130d3dff49d8",
   "metadata": {},
   "source": [
    "Ans'\n",
    "Gini Impurity and Entropy are both metrics used in decision trees to measure the impurity or disorder of a dataset, but they differ in their formulas and ranges. Gini Impurity uses a formula that results in a range of 0 to 0.5 for binary classification, while Entropy uses a logarithmic formula that ranges from 0 to 1. Gini Impurity is computationally faster because it avoids logarithmic calculations, making it a preferred choice for large datasets, though Entropy's results are sometimes considered slightly better. [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  \n",
    "\n",
    "| Feature | Gini Impurity | Entropy  |\n",
    "| --- | --- | --- |\n",
    "| Formula | Measures the probability of misclassification when randomly picking an element and assigning it a label according to the distribution of labels in the node. | Measures the disorder or randomness of a system, representing the uncertainty in a statistical sense.  |\n",
    "| Range | $0$ to $0.5$ for binary classification. | $0$ to $1$.  |\n",
    "| Computational Cost | Less expensive to compute because it does not involve logarithms. | More expensive to compute due to the use of logarithmic functions.  |\n",
    "| Interpretation | A Gini impurity of $0$ means a pure node (all data points belong to one class), while a value of $0.5$ indicates the maximum impurity for a binary classification. | An entropy of $0$ means a pure node, while a value of $1$ (or the maximum, $\\log_2 C$ for $C$ classes) indicates maximum impurity.  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506cbc0-c7c0-452e-943d-baa5337f844e",
   "metadata": {},
   "source": [
    "Question 3:What is Pre-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59645792-e407-45f0-b37a-2ccf66a41002",
   "metadata": {},
   "source": [
    "Ans.\n",
    "    Pre-pruning, or early stopping, is a technique in decision trees that halts the growth of a tree during its construction to prevent overfitting. This is achieved by setting conditions that stop the tree-building process before it becomes too complex, such as limiting the maximum depth, requiring a minimum number of samples per node, or stopping when a split no longer provides significant impurity decrease.\n",
    "\n",
    "    How it works\n",
    "Stopping the growth: Pre-pruning involves stopping the tree from growing beyond a certain point, rather than waiting for it to be fully built and then trimming it. \n",
    "Setting conditions: Conditions are set beforehand to determine when to stop. These can include: \n",
    "Maximum depth: Limiting the maximum number of layers in the tree. \n",
    "Minimum samples per leaf: Stopping when the number of samples in a node is below a certain threshold. \n",
    "Minimum samples per split: Requiring a minimum number of samples to split a node. \n",
    "Minimum impurity decrease: Halting the split if it does not meet a minimum decrease in impurity, such as Gini impurity or information gain. \n",
    "Preventing overfitting: By stopping the tree early, it prevents the model from becoming too complex and memorizing noise in the training data, which leads to better generalization on unseen data. \n",
    "Risk of underfitting: A potential drawback is that stopping the tree too early can lead to underfitting, where the model is too simple to capture the underlying patterns in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f8da1-f31f-4829-84ad-ab975e416747",
   "metadata": {},
   "source": [
    "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
    "Impurity as the criterion and print the feature importances (practical).\n",
    "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7199ed98-60a5-4a46-a2c3-a2a7a7d0cc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n",
      "\n",
      "Model Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X = data.data          # Features\n",
    "y = data.target        # Target labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the Decision Tree Classifier using Gini impurity\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(data.feature_names, clf.feature_importances_):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Evaluate accuracy on test data\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448fd0f-c587-43e0-9227-0e376ed1486c",
   "metadata": {},
   "source": [
    "Question 5: What is a Support Vector Machine (SVM)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfe821-b1e9-40c7-8173-bc57a57a45da",
   "metadata": {},
   "source": [
    "Ans.\n",
    "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks, but it is mainly used for classification.\n",
    "\n",
    "üîπ Concept:\n",
    "\n",
    "SVM works by finding the best decision boundary (called a hyperplane) that separates data points of different classes with the maximum margin.\n",
    "\n",
    "The margin is the distance between the hyperplane and the nearest data points from each class.\n",
    "\n",
    "These nearest points are called Support Vectors ‚Äî they are the most important data points that define the boundary.\n",
    "\n",
    "üîπ Key Idea:\n",
    "\n",
    "SVM aims to maximize the margin between classes while minimizing classification error.\n",
    "\n",
    "This makes the classifier more robust and generalizable to new data.\n",
    "\n",
    "üîπ Mathematical Form:\n",
    "\n",
    "For a binary classification problem, the hyperplane can be written as:\n",
    "\n",
    "ùë§\n",
    "ùëá\n",
    "ùë•\n",
    "+\n",
    "ùëè\n",
    "=\n",
    "0\n",
    "w\n",
    "T\n",
    "x+b=0\n",
    "\n",
    "Where:\n",
    "\n",
    "ùë§\n",
    "w ‚Üí weight vector (defines orientation of the hyperplane)\n",
    "\n",
    "ùëè\n",
    "b ‚Üí bias term (defines offset from origin)\n",
    "\n",
    "ùë•\n",
    "x ‚Üí input features\n",
    "\n",
    "üîπ Types of SVM:\n",
    "\n",
    "Linear SVM:\n",
    "Used when data is linearly separable.\n",
    "‚Üí Example: separating two classes with a straight line.\n",
    "\n",
    "Non-Linear SVM:\n",
    "Used when data is not linearly separable.\n",
    "‚Üí Applies the Kernel Trick to map data into higher dimensions.\n",
    "\n",
    "üîπ Advantages:\n",
    "\n",
    "Works well on high-dimensional data.\n",
    "\n",
    "Effective even with small datasets.\n",
    "\n",
    "Can handle non-linear relationships using kernel functions.\n",
    "\n",
    "üîπ Disadvantages:\n",
    "\n",
    "Not ideal for large datasets (training is slow).\n",
    "\n",
    "Performance depends on choice of kernel and parameter tuning.\n",
    "\n",
    "Hard to interpret compared to simpler models like Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57326668-756c-440d-a2bc-0a85f8f369e0",
   "metadata": {},
   "source": [
    "\n",
    "Question 6: What is the Kernel Trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727d934-dfe6-4018-99fd-c16aafd01cf9",
   "metadata": {},
   "source": [
    "Ans.\n",
    "The Kernel Trick in Support Vector Machines (SVM) is a technique that allows the algorithm to classify data that is not linearly separable by transforming it into a higher-dimensional space ‚Äî without explicitly computing the transformation.\n",
    "\n",
    "üîπ Explanation:\n",
    "\n",
    "Some datasets cannot be separated by a straight line (in 2D) or a hyperplane (in higher dimensions).\n",
    "\n",
    "The kernel function implicitly maps the input data into a higher-dimensional feature space where it becomes linearly separable.\n",
    "\n",
    "Instead of computing this transformation directly (which is computationally expensive), the kernel trick computes the dot product of the transformed features using a kernel function\n",
    "| Kernel Type                                       | Mathematical Form                     | Description                           |       |   |       |                                                                           |\n",
    "| ------------------------------------------------- | ------------------------------------- | ------------------------------------- | ----- | - | ----- | ------------------------------------------------------------------------- |\n",
    "| **Linear Kernel**                                 | ( K(x, y) = x^T y )                   | Used when data is linearly separable. |       |   |       |                                                                           |\n",
    "| **Polynomial Kernel**                             | ( K(x, y) = (x^T y + c)^d )           | Allows curved decision boundaries.    |       |   |       |                                                                           |\n",
    "| **RBF (Radial Basis Function) / Gaussian Kernel** | ( K(x, y) = e^{-\\gamma                |                                       | x - y |   | ^2} ) | Maps data to infinite dimensions ‚Äî very powerful for non-linear problems. |\n",
    "| **Sigmoid Kernel**                                | ( K(x, y) = \\tanh(\\alpha x^T y + c) ) | Similar to neural network activation. |       |   |       |                                                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e58b04-d7cc-48f5-b895-c210bb08cf3c",
   "metadata": {},
   "source": [
    "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
    "kernels on the Wine dataset, then compare their accuracies.\n",
    "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
    "on the same dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27313e7-cf7e-4074-8260-74a2b522282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Accuracies on Wine Dataset:\n",
      "\n",
      "Linear Kernel Accuracy: 0.9815\n",
      "RBF Kernel Accuracy:    0.7593\n",
      "\n",
      "‚û° Linear Kernel performed better.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "data = load_wine()\n",
    "X = data.data          # Features\n",
    "y = data.target        # Labels\n",
    "\n",
    "# Split the dataset into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create two SVM classifiers: one with Linear kernel and one with RBF kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Train both models\n",
    "svm_linear.fit(X_train, y_train)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Print comparison results\n",
    "print(\"SVM Classifier Accuracies on Wine Dataset:\\n\")\n",
    "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
    "print(f\"RBF Kernel Accuracy:    {acc_rbf:.4f}\")\n",
    "\n",
    "# Determine which performed better\n",
    "if acc_linear > acc_rbf:\n",
    "    print(\"\\n‚û° Linear Kernel performed better.\")\n",
    "elif acc_linear < acc_rbf:\n",
    "    print(\"\\n‚û° RBF Kernel performed better.\")\n",
    "else:\n",
    "    print(\"\\n‚û° Both kernels performed equally well.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde899d-936f-4c5b-9431-f01ebc26426d",
   "metadata": {},
   "source": [
    "Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b5f15-36bf-4d6f-a503-e8d55b80c5be",
   "metadata": {},
   "source": [
    "A Na√Øve Bayes Classifier is a supervised machine learning algorithm based on Bayes‚Äô Theorem, used mainly for classification problems (especially text classification, like spam detection, sentiment analysis, etc.).\n",
    "\n",
    "üîπ Bayes‚Äô Theorem:\n",
    "P(A‚à£B)=P(B)P(B‚à£A)‚ãÖP(A)\n",
    "Where:\n",
    "P(A‚à£B): Probability of class \n",
    "A given data B (posterior probability)\n",
    "P(B‚à£A): Probability of data \n",
    "B given class \n",
    "A (likelihood)\n",
    "P(A): Prior probability of class A\n",
    "P(B): Probability of data B\n",
    "üîπ How it works:\n",
    "\n",
    "The algorithm calculates the probability of each class given the input features and then predicts the class with the highest probability.\n",
    "\n",
    "It assumes that all features are independent of each other given the class label ‚Äî this is where the term \"Na√Øve\" comes from.\n",
    "\n",
    "üîπ Why it is called ‚ÄúNa√Øve‚Äù:\n",
    "\n",
    "It‚Äôs called Na√Øve because it assumes independence between all input features, meaning that the presence of one feature does not affect the presence of another.\n",
    "\n",
    "In reality, this assumption is often false ‚Äî but surprisingly, the algorithm still performs very well in many real-world tasks.\n",
    "| Type                        | Description                                                             |\n",
    "| --------------------------- | ----------------------------------------------------------------------- |\n",
    "| **Gaussian Na√Øve Bayes**    | Assumes features follow a normal (Gaussian) distribution.               |\n",
    "| **Multinomial Na√Øve Bayes** | Used for discrete features like word counts in text classification.     |\n",
    "| **Bernoulli Na√Øve Bayes**   | Used for binary/boolean features (e.g., presence or absence of a word). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d317260-fce7-477f-9baa-705b4961b214",
   "metadata": {},
   "source": [
    "Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve\n",
    "Bayes, and Bernoulli Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95910162-d8ab-4f53-8941-38fbec7d80e8",
   "metadata": {},
   "source": [
    "Ans.Na√Øve Bayes algorithms come in several variants, depending on the type of data and the assumptions about how the features are distributed.\n",
    "The three main types are Gaussian, Multinomial, and Bernoulli Na√Øve Bayes.\n",
    "\n",
    "üîπ 1. Gaussian Na√Øve Bayes\n",
    "\n",
    "Used for: Continuous (numeric) data\n",
    "\n",
    "Assumption:\n",
    "Each feature follows a Normal (Gaussian) distribution.\n",
    "\n",
    "Example:\n",
    "Predicting whether a person has a disease based on continuous features like age, blood pressure, or cholesterol level.\n",
    "\n",
    "Formula:\n",
    "P(xi‚Äã‚à£y)=2œÄœÉy2‚Äã\n",
    "‚Äã1‚Äãe‚àí2œÉy2‚Äã(xi‚Äã‚àíŒºy‚Äã)2‚Äã\n",
    "where \n",
    "\n",
    " Œºy and œÉy‚Äã are the mean and standard deviation of feature xi and y\n",
    "\n",
    "\n",
    "Library Implementation:\n",
    "     from sklearn.naive_bayes import GaussianNB\n",
    " üîπ 2. Multinomial Na√Øve Bayes\n",
    "\n",
    "Used for: Discrete (count-based) data, especially text classification\n",
    "\n",
    "Assumption:\n",
    "Features represent counts or frequencies of events (e.g., number of times a word appears in a document).\n",
    "\n",
    "Example:\n",
    "Email spam detection, document classification, sentiment analysis.\n",
    "  formula :\n",
    "  \n",
    "                              P(xi‚Äã‚à£y)=(Ny‚Äã+Œ±n)(Nyi‚Äã+Œ±)‚Äã\n",
    "üîπ 3. Bernoulli Na√Øve Bayes\n",
    "\n",
    "Used for: Binary/boolean features (presence or absence)\n",
    "\n",
    "Assumption:\n",
    "Each feature is binary (1 = feature present, 0 = absent).\n",
    "\n",
    "Example:\n",
    "Whether a document contains certain keywords or not.\n",
    "\n",
    "Use Case:\n",
    "Good for datasets where only the presence of a feature matters, not its frequency.\n",
    "\n",
    "Library Implementation:\n",
    "from sklearn.naive_bayes import BernoulliNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c49ac1-9866-4b57-b613-8e80028938f6",
   "metadata": {},
   "source": [
    "Question 10: Breast Cancer Dataset\n",
    "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
    "dataset and evaluate accuracy.\n",
    "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
    "sklearn.datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c751fffa-dacd-4bd5-89ee-a2362856ddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gaussian Na√Øve Bayes Classifier on Breast Cancer Dataset\n",
      "\n",
      "Model Accuracy: 0.9415\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.93      0.90      0.92        63\n",
      "      benign       0.95      0.96      0.95       108\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.94       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data          # Features\n",
    "y = data.target        # Labels\n",
    "\n",
    "# Split the dataset into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create the Gaussian Na√Øve Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train (fit) the model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"‚úÖ Gaussian Na√Øve Bayes Classifier on Breast Cancer Dataset\\n\")\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\\n\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef61dfd-ac8c-4d6c-a80e-132c025a6936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
